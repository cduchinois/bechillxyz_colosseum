# BeChill - Web Application

BeChill is a web application that helps users manage their crypto portfolio with peace of mind. It offers a user-friendly interface with a virtual coach named "CHILL" that provides personalized advice to optimize your investments.

## ‚ö†Ô∏è Development Status

**IMPORTANT: This is currently a development version, not ready for production!**

- The application currently requires Ollama running locally for AI functionality
- Online AI integration is in progress but not yet implemented
- The AI model and responses still need significant improvements
- Many features are still under development or may not work as expected

## üöÄ Quick Start for Developers

Follow these steps to install and run the BeChill application on your local machine for development purposes.

### Prerequisites

- [Node.js](https://nodejs.org/) v16 or higher
- npm (included with Node.js)
- A [Convex](https://www.convex.dev/) account for the database
- [Ollama](https://ollama.ai/) installed locally for AI functionality
- API keys for external services (detailed in the Configuration section)

### Ollama Setup

1. **Install Ollama**
   - Download and install Ollama from [https://ollama.ai/download](https://ollama.ai/download)
   - Follow the installation instructions for your operating system

2. **Pull a compatible model**
   ```bash
   ollama pull llama3:8b
   # or any other compatible model you want to use
   ```

3. **Ensure Ollama is running**
   ```bash
   ollama serve
   ```
   Ollama should be accessible at http://localhost:11434 by default

### Installation

1. **Clone the repository**

```bash
git clone https://github.com/your-username/BeChill-web.git
cd BeChill-web/frontend/web    
or     
cd BeChill-web/frontend/mobile   
```

2. **Install dependencies**

```bash
npm install
```

### Configuration

1. **Create a .env.local file at the root of the project**

```
# Convex Configuration
CONVEX_DEPLOYMENT=your-deployment-id

# API Configuration
NEXT_PUBLIC_API_BASE_URL=http://localhost:3000
SOLSCAN_API_KEY=your-solscan-key

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b

# Additional environment variables if needed
```

2. **Get a Convex deployment ID**

If you haven't set up Convex yet, run:

```bash
npx convex login
```

Follow the instructions to connect to your Convex account.

### Starting the development server

To launch the application, you need to start both the Convex backend and the Next.js frontend server.

1. **Ensure Ollama is running**

Make sure your Ollama instance is active and the selected model is available.

2. **Start the Convex backend**

```bash
npx convex dev
```

This will:
- Deploy your Convex functions and database schema
- Watch for changes to your Convex files
- Provision a development deployment and save its name in .env.local

3. **In another terminal, start the Next.js frontend**

```bash
npm run dev
```

4. Install solscan api
```bash
cd ../services/solscan
```

```bash
npm install
```

```bash
npm run start
```

5. **Access the application**

Open [http://localhost:3000](http://localhost:3000) in your browser.

6. Now all was installed ans setup, next time, only go on frontend/web and run to start all servers:
```bash
npm run startAll
```

## üèóÔ∏è Project Structure

```
BeChill-web/
‚îú‚îÄ‚îÄ convex/                # Convex backend (schema, mutations, queries)
‚îÇ   ‚îú‚îÄ‚îÄ _generated/       # Files automatically generated by Convex
‚îÇ   ‚îú‚îÄ‚îÄ schema.ts         # Database schema
‚îÇ   ‚îú‚îÄ‚îÄ messages.ts       # Message-related functions
‚îÇ   ‚îî‚îÄ‚îÄ profile.ts        # User profile functions
‚îú‚îÄ‚îÄ public/               # Static files
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/              # Next.js routes and pages
‚îÇ   ‚îú‚îÄ‚îÄ components/       # Reusable React components
‚îÇ   ‚îî‚îÄ‚îÄ api/              # Next.js API routes
‚îî‚îÄ‚îÄ package.json          # Dependencies and scripts
```

## üìù Known Limitations

- **Local AI Only**: Currently, the AI functionality requires Ollama running locally. Online integration is in progress.
- **AI Model Quality**: The AI responses are still being refined and may not always be relevant or accurate.
- **Limited Features**: Many planned features are not yet implemented.
- **Test Profiles**: The application uses test profiles for demonstration purposes.

## üíª Developer Notes

- Code is still in active development and may contain bugs or incomplete features
- The chat interface works with session-based history that resets on page refresh
- Portfolio analysis features are partially implemented but may use mock data
- Work is in progress to integrate with a cloud-based AI solution instead of requiring local Ollama

## üõ†Ô∏è Troubleshooting

### Convex Connection Issues

If you encounter errors connecting to Convex:

1. Check that `npx convex dev` is running
2. Make sure the `.env.local` file contains the correct value for `CONVEX_DEPLOYMENT`
3. Try resetting your deployment: `npx convex logout` then `npx convex login`

### Ollama Issues

If AI responses are not working:

1. Verify Ollama is running (`ollama serve`)
2. Check that you've pulled the correct model (`ollama list`)
3. Ensure the OLLAMA_BASE_URL and OLLAMA_MODEL in your .env.local are correct
4. Look for errors in the server logs

### API Issues

If portfolio analysis features are not working:

1. Check that your API keys are correctly configured in `.env.local`
2. Make sure external services (Solscan) are operational


